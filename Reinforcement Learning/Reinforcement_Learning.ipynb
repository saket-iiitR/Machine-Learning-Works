{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reinforcement Learning</h1>\n",
    "<h6>Saket Tiwari</h6>\n",
    "Date: 08 July 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isme humlog machine ko sikhaynge ki kab kon sa action perform karna\n",
    "#reinforcement me humlog model ko btaynge ki action jo liya h wo  sahi h ya galat humlog ko output nhi pata, reward btaynge\n",
    "\n",
    "\n",
    "#state t par koi action perform kiye to wo state (t+1) me jayga.. lekin jo reward milega humlogon ko wo (t+1) ka reward milega\n",
    "\n",
    "# alpha -  learning rate\n",
    "# gamma - discount factor\n",
    "# epsilon - exploration rate\n",
    "\n",
    "#agar game me(5x4) 20 pos honge to q table me rows 20 hoga\n",
    "#Q-learning -> Q-table -> rows- describes states, columns-action\n",
    "#formula ->  Q(s_t , a_t)= r_(t+1) +   gamma* r_(t+2) + gamma^2 * r_(T+3)\n",
    "#future me jo reward milne wala h usko hmlog disregard karte jaynge kyuki hmlog future me confirm nhi h\n",
    "# ..ki wo reward milega ki nahi humlogon ko\n",
    "#we are concerned with overall reward not the immediate reward\n",
    "\n",
    "#Q(s_t , a_t)= r_(t+1) +   gamma* r_(t+2) + gamma^2 * r_(T+3)\n",
    "# Q(s_(t+1), a_(t+1)) = r_(t+2) +gamma*r_(t+3) + gamma^2 *r_(t+4)\n",
    "\n",
    "#Therefor final reward = Q(s_t , a_t) = r_(t+1) + gamma *Q(s_(t+1), a_(t+1))\n",
    "# or  Q(s_t , a_t) = r_(t+1) +gamma * Q_max_(s_(t+1))   Q_max me  future me jitna reward milte wala h\n",
    "\n",
    "#exploration rate explores the other states for highest reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frozen ice -> slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/gym/blob/master/gym/envs/__init__.py par ja kr randomness off krnge\n",
    "\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNoSlip-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery':False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'FrozenLakeNoSlip-v0'\n",
    "env= gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.isDiscrete =( type( env.action_space)) ==gym.spaces.discrete.Discrete\n",
    "        \n",
    "    def get_action(self,state):#check kareha ki discrete ya continuous\n",
    "        \n",
    "        if self.isDiscrete:\n",
    "            action = random.choice(range(env.action_space.n)) \n",
    "        else:\n",
    "            action = np.random.uniform( env.action_space.low,\n",
    "                                      env.action_space.high,\n",
    "                                      env.action_space.shape) #because it is a continuous h\n",
    "        return action\n",
    "\n",
    "    \n",
    "agent= Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "total_reward=0\n",
    "for ep in range(10):\n",
    "    \n",
    "    state= env.reset() # game fresh start hoga\n",
    "    done=False # gave abhi over nahi hua h\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action= agent.get_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, env, discountedRate=0.97, learningRate=0.01):\n",
    "        \n",
    "        self.isDiscrete =( type( env.action_space)) ==gym.spaces.discrete.Discrete\n",
    "        self.stateSize=env.observation_space.n\n",
    "        self.actionSize = env.action_space.n\n",
    "        self.explorationRate=1.0\n",
    "        self.discountedRate= discountedRate\n",
    "        self.learningRate= learningRate\n",
    "        self.qTable= 1e-4*np.random.random([self.stateSize, self.actionSize])\n",
    "        \n",
    "    def get_action(self,state):#check kareha ki discrete ya continuous\n",
    "        \n",
    "        qState= self.qTable[state] # we will get a single row with corresponding state #sbse pehle kaun se state pr h\n",
    "        actionGreedy=np.argmax(qState)     \n",
    "        actionRandom=env.action_space.sample()\n",
    "        \n",
    "        return actionRandom if random.random()<self.explorationRate else actionGreedy\n",
    "    \n",
    "    def train(self, state, action, next_state , reward, done): #(state, action, next_state , reward, done) is called experience\n",
    "        qStateNext=self.qTable[next_state]\n",
    "        if done:\n",
    "            qStateNext = np.zeros([self.actionSize])\n",
    "        else:\n",
    "            qStateNext= self.qTable[next_state]\n",
    "        qTarget = reward + self.discountedRate*np.max(qStateNext)\n",
    "        qUpdate = qTarget - self.qTable[state, action]\n",
    "        self.qTable[state, action]+=self.learningRate*qUpdate\n",
    "        \n",
    "        if done:\n",
    "            self.explorationRate*=0.99\n",
    "        \n",
    "agent= Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:15,a:2\n",
      "Ep:99,Goals:10.0, Explore:0.36603234127322926\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "[[8.38348880e-05 8.50280486e-05 4.32886961e-05 4.42219907e-05]\n",
      " [6.16248146e-05 1.18628906e-05 7.60024896e-05 6.51180528e-05]\n",
      " [4.49707679e-05 8.35571330e-05 6.88482052e-05 6.91467659e-05]\n",
      " [4.92823556e-05 2.77991778e-05 9.28140945e-05 1.45866569e-05]\n",
      " [8.60915995e-05 8.31077326e-05 6.30356860e-05 2.76906476e-05]\n",
      " [5.24200642e-05 7.99565469e-06 1.45887604e-05 8.36816923e-06]\n",
      " [5.36613061e-05 4.59071319e-05 7.77647646e-05 1.16657799e-05]\n",
      " [6.52489991e-06 6.40866490e-05 4.63366484e-06 3.68075341e-05]\n",
      " [2.94979113e-05 6.83794444e-05 9.25961514e-05 4.50018394e-05]\n",
      " [4.37225205e-05 9.20260749e-05 2.61571505e-04 9.26948989e-05]\n",
      " [6.00667802e-05 3.99719271e-03 8.61110515e-05 8.38828547e-05]\n",
      " [7.53533889e-05 6.23302883e-05 1.81477396e-05 5.47302525e-05]\n",
      " [8.07319327e-05 2.16822646e-05 8.56134600e-05 1.20086852e-05]\n",
      " [5.00770479e-05 1.60978761e-07 1.19930572e-03 1.83869439e-06]\n",
      " [9.53734872e-06 9.91226337e-04 9.56840236e-02 7.41995210e-05]\n",
      " [6.82004625e-05 2.74506687e-05 7.50571792e-05 8.70376562e-05]]\n"
     ]
    }
   ],
   "source": [
    "total_reward=0\n",
    "for ep in range(100):  \n",
    "    state= env.reset() # game fresh start hoga\n",
    "    done=False # gave abhi over nahi hua h\n",
    "    \n",
    "    while not done:\n",
    "        action= agent.get_action(state)\n",
    "        nextState, reward, done, info = env.step(action)\n",
    "        agent.train(state,action, nextState,reward,done)\n",
    "        state=nextState\n",
    "        total_reward+=reward\n",
    "        print(f\"s:{state},a:{action}\")\n",
    "        print(f'Ep:{ep},Goals:{total_reward}, Explore:{agent.explorationRate}')\n",
    "        env.render()\n",
    "        print(agent.qTable)\n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, env, discountedRate=0.97, learningRate=0.01):\n",
    "        \n",
    "        self.isDiscrete =( type( env.action_space)) ==gym.spaces.discrete.Discrete\n",
    "        self.stateSize=env.observation_space.n\n",
    "        self.actionSize = env.action_space.n\n",
    "        \n",
    "        self.explorationRate=1.0\n",
    "        self.discountedRate= discountedRate\n",
    "        self.learningRate= learningRate\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        #creating variables\n",
    "        self.stateIn =tf.placeholder(tf.int32,shape=[1])\n",
    "        self.actionIn =tf.placeholder(tf.int32,shape=[1])\n",
    "        self.targetIn =tf.placeholder(tf.float32,shape=[1])\n",
    "        \n",
    "        self.state =tf.one_hot(self.stateIn, depth=self.stateSize)\n",
    "        self.action =tf.one_hot(self.actionIn, depth=self.actionSize)\n",
    "        self.qState= tf.layers.dense(self.state, units= env.action_space.n, name='Qtable')\n",
    "        self.qAction = tf.reduce_sum(tf.multiply(self.qState, self.action), axis=1) #Scalar\n",
    "        \n",
    "        self.loss=tf.reduce_sum(tf.square(self.targetIn-self.qAction))\n",
    "        self.optimizer=tf.train.AdamOptimizer(self.learningRate).minimize(self.loss)\n",
    "        self.sess=tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def get_action(self,state):#check kareha ki discrete ya continuous\n",
    "        \n",
    "        qState= self.sess.run(self.qState,feed_dict= {self.stateIn:[state]}) # we will get a single row with corresponding state #sbse pehle kaun se state pr h\n",
    "        actionGreedy=np.argmax(qState)     \n",
    "        actionRandom=env.action_space.sample()\n",
    "        \n",
    "        return actionRandom if random.random()<self.explorationRate else actionGreedy\n",
    "    \n",
    "    def train(self, state, action, next_state , reward, done): #(state, action, next_state , reward, done) is called experience\n",
    "        \n",
    "        if done:\n",
    "            qStateNext = np.zeros([self.actionSize])\n",
    "        else:\n",
    "            qStateNext= self.sess.run(self.qState, feed_dict= {self.stateIn:[next_state] })\n",
    "        qTarget = reward + self.discountedRate*np.max(qStateNext)\n",
    "        \n",
    "        \n",
    "        feed ={ self.stateIn:[state],self.actionIn:[action], self.targetIn:[qTarget]}\n",
    "        self.sess.run(self.optimizer, feed_dict=feed)\n",
    "        if done:\n",
    "            self.explorationRate*=0.99\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.sess.close()\n",
    "        \n",
    "agent= Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:15,a:2\n",
      "Ep:99,Goals:60.0, Explore:0.13397967485796175\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "[[ 3.0034229e-01  1.5514053e-01  9.9149972e-02  1.4712311e-03]\n",
      " [ 2.9294816e-01 -5.4990214e-01  1.2902808e-01  1.3375770e-01]\n",
      " [ 3.4233361e-01  3.5908777e-01  5.2457903e-02  1.6361105e-01]\n",
      " [ 3.2497284e-01 -3.1679502e-01 -5.6305647e-02 -3.0368692e-03]\n",
      " [ 5.7104278e-02  1.4110394e-01 -6.3415742e-01  8.9487545e-02]\n",
      " [ 6.9905162e-02  1.0132694e-01 -1.0720506e-01 -7.2537959e-02]\n",
      " [-5.9418625e-01  3.7649924e-01 -7.7506649e-01  1.7263173e-01]\n",
      " [-9.5067620e-03 -2.0048982e-01 -4.7103602e-01 -1.1449993e-01]\n",
      " [ 1.5990813e-01 -3.8714001e-01 -1.7517290e-04  4.4217922e-02]\n",
      " [ 1.5699953e-02  3.8959095e-01 -3.4888476e-02 -5.6483757e-01]\n",
      " [ 4.2141238e-01  4.1830918e-01 -4.9695361e-01  1.9285630e-01]\n",
      " [-1.0826334e-01 -2.2063705e-01  2.0794761e-01 -1.6201201e-01]\n",
      " [ 4.8506117e-01  1.9182444e-01  4.3048787e-01  1.8552858e-01]\n",
      " [-6.5426648e-01  1.4091173e-01  2.1401879e-01  1.7491673e-01]\n",
      " [ 4.3396807e-01  4.3527949e-01  2.4507011e-01  2.1868190e-01]\n",
      " [ 2.3545861e-01  2.2130251e-02  2.1259862e-01  4.4044465e-01]]\n"
     ]
    }
   ],
   "source": [
    "total_reward=0\n",
    "for ep in range(100):  \n",
    "    state= env.reset() # game fresh start hoga\n",
    "    done=False # gave abhi over nahi hua h\n",
    "    \n",
    "    while not done:\n",
    "        action= agent.get_action(state)\n",
    "        nextState, reward, done, info = env.step(action)\n",
    "        agent.train(state,action, nextState,reward,done)\n",
    "        state=nextState\n",
    "        total_reward+=reward\n",
    "        print(f\"s:{state},a:{action}\")\n",
    "        print(f'Ep:{ep},Goals:{total_reward}, Explore:{agent.explorationRate}')\n",
    "        env.render()\n",
    "        \n",
    "        with tf.variable_scope('Qtable', reuse=True): #if reuse=True weights will not reset.\n",
    "            weights=agent.sess.run(tf.get_variable('kernel'))\n",
    "        print(weights)\n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
